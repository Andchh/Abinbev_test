# üöÄ Projeto Airflow com Docker

Este projeto utiliza o Apache Airflow em um ambiente Dockerizado para orquestra√ß√£o de pipelines de dados. √â ideal para desenvolvimento local, testes e aprendizado.
Este projeto utiliza Apache Airflow  e Spark em um ambiente Docker para orquestra√ß√£o de pipeline de dados.
Ele foi criado para o desafio t√©cnico para a empresa AbInbev.

O airflow foi escolhido por ser uma ferramenta conhecida tanto na √°rea de individual como em grandes empresas, facilitando uma simula√ß√£o para um mundo corporativo.
O docker permite que o projeto que foi criado e configurado de um jeito espec√≠fico possa rodar em qualquer m√°quina com o suporte para o mesmo, facilitando o compartilhamento de c√≥digo. Tamb√©m √© preciso considerar que sua rede interna pode ser configurada para simular a comunica√ß√£o entre componentes geralmente usados para solu√ß√µes de dados. Nesse projeto, por exemplo, o airflow poderia mandar o comando para o spark via rede local docker.
Python e Spark foram escolhidos como linguagens dessa solu√ß√£o por serem mais utilizadas no ramo de engenharia de dados. O python √© usado para comunica√ß√£o com a API, para mostrar sua efici√™ncia como linguagem de script e o Spark √© usado para tratamento de dados. No docker compose desse projeto tamb√©m √© poss√≠vel criar diferentes workers para o spark, simulando uma rede com v√°rios computadores trabalhando em paralelo para computar a solu√ß√£o. 

## üìÅ Estrutura do Projeto

```
.
‚îú‚îÄ‚îÄ airflow/              # Configura√ß√µes e arquivos internos do Airflow
‚îú‚îÄ‚îÄ config/               # Configura√ß√µes gerais do projeto
‚îú‚îÄ‚îÄ dags/                 # DAGs do Airflow
‚îú‚îÄ‚îÄ logs/                 # Logs gerados pelo Airflow
‚îú‚îÄ‚îÄ notebooks/            # Notebooks Jupyter (opcional)
‚îú‚îÄ‚îÄ plugins/              # Plugins personalizados para o Airflow
‚îú‚îÄ‚îÄ spark/                # Scripts ou configs relacionados ao Apache Spark
‚îú‚îÄ‚îÄ var/                  # Dados vari√°veis usados no projeto
‚îú‚îÄ‚îÄ .env                  # Vari√°veis de ambiente
‚îú‚îÄ‚îÄ docker-compose.yml    # Orquestra√ß√£o de containers
‚îú‚îÄ‚îÄ Dockerfile            # Constru√ß√£o da imagem customizada
‚îú‚îÄ‚îÄ requirements.txt      # Depend√™ncias adicionais do Python
‚îî‚îÄ‚îÄ .gitignore            # Arquivos e pastas ignoradas pelo Git
```

## ‚úÖ Pr√©-requisitos

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)

## ‚öôÔ∏è Configura√ß√£o

### 1. Clone o reposit√≥rio
Crie uma pasta chamada brewer, execute o terminal dentro dela e clone o reposit√≥rio com: 
```bash
git clone https://github.com/Andchh/brewer.git
cd brewer
```
Isso √© importante para fins de compatibilidade e vai evitar erros por nome de volume.


### 2. Crie o arquivo `.env`

Voc√™ pode copiar um modelo existente ou criar manualmente um arquivo de texto e nome√°-lo .env dentro da pasta brewer que voc√™ baixou. Exemplo:

```bash
AIRFLOW_UID=50000
```
Por√©m o projeto j√° ter√° um arquivo .env pr√© definido com esse valor, considerando uma m√°quina com sistema operacional do Windows.
> Obs: `AIRFLOW_UID` pode ser obtido com `id -u` no Linux/macOS, ou definido como `50000` no Windows.

### 3. Suba os containers

```bash
docker-compose up --build
```

A primeira vez pode demorar alguns minutos, pois as imagens ser√£o constru√≠das e depend√™ncias instaladas.

### 4. Acesse o Airflow

Ap√≥s os containers estarem rodando, acesse:

```
http://localhost:8080
```

Login padr√£o:

- **Usu√°rio:** `airflow`
- **Senha:** `airflow`

### 5. Instalando depend√™ncias adicionais (opcional)

Adicione as bibliotecas necess√°rias ao `requirements.txt`, e elas ser√£o instaladas no container do Airflow.

---

## üõ† Comandos √∫teis

| Comando                          | Descri√ß√£o                          |
|----------------------------------|------------------------------------|
| `docker-compose up`              | Inicia os containers               |
| `docker-compose down`            | Para os containers                 |
| `docker-compose logs -f`         | Visualiza logs em tempo real       |
| `docker-compose exec airflow-apiserver bash` | Acessa o terminal do Airflow     |
---
Esses comando podem variar baseado no nome da sua pasta. O Airflow pode criar, por exemplo, o airflow-apiserver como seufolder_airflow-apiserver. O projeto foi criado pensado em n√£o depender desses nomes, por√©m √© interessante saber que pode dar esse problema.

## üßº Reset do ambiente (‚ö†Ô∏è apaga dados)

```bash
docker-compose down -v
```

---

## Sobre o projeto e como testar

O projeto cria 3 Dags: 
* create_bronze_breweries

	Essa dag vai ler os dados da API em formato Json e salv√°-los na pasta: ./airflow/datalake/bronze. O docker est√° configurado para copiar os arquivos desta pasta para o volume interno "datalake-volume".
	
* create_silver_breweries

	Esta dag vai esperar o fim da dag create_bronze_breweries e executar uma limpeza nos dados. As informa√ß√µes de localiza√ß√£o (pa√≠s, estados e cidade) n√£o vem padronizadas, contendo assim abrevia√ß√µes, acentos ou tamanhos diferentes. Esse primeiro passo normaliza tudo para depois escrever os dados no formato colunar .parquet e particionado por localiza√ß√£o (pa√≠s, estado e cidade) na pasta .airflow/datalake/silver, que tamb√©m √© copiada para o volume interno "datalake-volume".
	
* create_gold_breweries

	Esta dag vai executar um script pyspark para gerar dados provindos de agrega√ß√µes. 
	Primeiro haver√° a agrega√ß√£o por localiza√ß√£o onde mostrar√° o n√∫mero de cervejarias por localiza√ß√£o (pa√≠s, estado e cidade).
	Logo ap√≥s o script tamb√©m faz uma agrega√ß√£o por tipo de cervejaria. 
	Em seguida o script faz uma agrega√ß√£o por tipo e localiza√ß√£o de cervejaria. 
	Assim, ao final de tudo os dados s√£o salvos em formato .parquet na pasta ./airflow/datalake/gold, que tamb√©m √© copiada para o volume interno "datalake-volume". 

---
## Como testar ap√≥s os dados escritos?

Para testes de dados √© poss√≠vel executar o spark-master dentro do docker, seguindo:

Execute um terminal dentro da pasta do projeto.
Procure o nome do spark-master com:
```bash
docker ps
```
Provavelmente vai aparecer como seufolder_spark-master-1. Para fins desse exemplo vamos usar brewer-spark-master-1 como exemplo.
Em seguida acesse o container do master com:

```bash
docker exec -it brewer-spark-master-1 bash
```
Isso vai nos proporcionar executar comandos bash dentro do container. Agora vamos ativar o pyspark com o comando simples:
```bash
pyspark
```
Agora podemos ler o arquivo .parquet com:
```bash
df = spark.read.parquet("/opt/airflow/datalake/gold/breweries_by_location")
```
Ou
```bash
df = spark.read.parquet("/opt/airflow/datalake/gold/breweries_by_type")
```
Ou
```bash
df = spark.read.parquet("/opt/airflow/datalake/gold/breweries_by_type_location")
```
E exibir os dados com:

```bash
df.show()
```

Pronto. 
Caso queira testar os dados das outras camadas √© s√≥ mudar o caminho do spark.read para /opt/airflow/datalake/silver ou /opt/airflow/datalake/bronze.
Lembrando que, como a camada bronze est√° totalmente em "crua" em formato .json, ent√£o temos que mudar a forma de leitura para:


```bash
df = spark.read.option("multiline", "true").option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record").json("/opt/airflow/datalake/bronze")
```
---
